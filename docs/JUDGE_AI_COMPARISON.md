# ü§ñ SISTEMA DE JUEZ VIRTUAL AI
## Comparaci√≥n y Optimizaci√≥n de Calificaciones con GLM-4V

---

## üìã TABLA DE CONTENIDOS

1. [Descripci√≥n del Sistema](#descripci√≥n-del-sistema)
2. [Configuraci√≥n Inicial](#configuraci√≥n-inicial)
3. [Flujo de Trabajo](#flujo-de-trabajo)
4. [Scripts Disponibles](#scripts-disponibles)
5. [Interpretaci√≥n de Resultados](#interpretaci√≥n-de-resultados)
6. [Optimizaci√≥n Continua](#optimizaci√≥n-continua)

---

## üéØ DESCRIPCI√ìN DEL SISTEMA

Este sistema permite:

‚úÖ **Analizar videos existentes** con GLM-4V (AI vision model)
‚úÖ **Comparar calificaciones** de jueces humanos vs juez AI
‚úÖ **Generar reportes visuales** con estad√≠sticas y gr√°ficos
‚úÖ **Detectar patrones de error** para mejorar el prompt
‚úÖ **Optimizar autom√°ticamente** el juez AI basado en feedback

**Objetivo:** Crear un juez virtual que se alinee lo m√°s posible con los criterios de los jueces humanos.

---

## ‚öôÔ∏è CONFIGURACI√ìN INICIAL

### **1. Agregar API Key de GLM**

```bash
# Editar .env.local
GLM_API_KEY=tu_api_key_aqui
```

**¬øC√≥mo obtener API key?**
1. Ir a: https://open.bigmodel.cn/
2. Registrarse (gratis con cr√©ditos iniciales)
3. Crear API key en el dashboard
4. Costo aprox: $0.001-0.01 por an√°lisis de video

### **2. Verificar modelo ScoutReport en schema**

El modelo `ScoutReport` debe existir en `prisma/schema.prisma`:

```prisma
model ScoutReport {
  id               Int      @id @default(autoincrement())
  skaterEmail      String
  videoUrl         String?
  analysis         Json
  techniqueScore   Int?
  olympicPotential Int?
  suggestedPrice   Decimal? @db.Decimal(10, 2)
  detectedTricks   String[]
  comparisonNotes  String?
  modelVersion     String?
  confidence       Decimal? @db.Decimal(3, 2)
  createdAt        DateTime @default(now())

  skater           User     @relation(fields: [skaterEmail], references: [email])

  @@index([skaterEmail])
  @@index([createdAt])
  @@map("scout_reports")
}
```

Ejecutar migrations si no existe:

```bash
npx prisma db push
```

### **3. Verificar que hay submissions para analizar**

```bash
node scripts/check-submissions.js
```

Este script verifica que tengas submissions con:
- `status: 'approved'`
- `score: NOT NULL` (calificaci√≥n humana)
- `videoUrl: NOT NULL`

---

## üîÑ FLUJO DE TRABAJO

### **Paso 1: Analizar submissions existentes**

```bash
node scripts/analyze-existing-submissions.js
```

**Qu√© hace:**
1. Obtiene las √∫ltimas 50 submissions aprobadas
2. Env√≠a cada video a GLM-4V para an√°lisis
3. Compara score humano vs AI
4. Guarda resultados en `ScoutReport`
5. Muestra estad√≠sticas en consola

**Salida esperada:**
```
üé¨ Analizando submissions existentes con AI...

[1/50] Analizando: Juan P√©rez - Ollie
  Video: https://youtube.com/watch?v=xxx
  Score humano: 75
  Score AI: 78
  Diferencia: 3 puntos (4.0%)
  Acuerdo: EXCELLENT

...

üìä REPORTE FINAL
Total submissions analizadas: 48
Diferencia promedio: 8.3 puntos
Diferencia promedio: 11.2%

Distribuci√≥n de acuerdos:
  EXCELLENT   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 30 (62.5%)
  GOOD         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 12 (25.0%)
  ACCEPTABLE   ‚ñà‚ñà‚ñà‚ñà 4 (8.3%)
  POOR         ‚ñà‚ñà‚ñà‚ñà 2 (4.2%)

‚úÖ EXCELENTE: El juez virtual est√° muy alineado con jueces humanos
```

### **Paso 2: Generar reporte visual**

```bash
node scripts/judge-comparison-report.js
```

**Qu√© hace:**
- Genera archivo HTML `judge-ai-comparison-report.html`
- Contiene gr√°ficos interactivos con Chart.js
- Tablas detalladas con todas las comparaciones
- An√°lisis por juez y por dificultad

**Abrir reporte:**
```bash
# En Windows
start judge-ai-comparison-report.html

# En Mac
open judge-ai-comparison-report.html

# En Linux
xdg-open judge-ai-comparison-report.html
```

**El reporte incluye:**
- üìä Gr√°fico de distribuci√≥n de acuerdos
- üìà Scatter plot comparando scores
- üéØ An√°lisis por nivel de dificultad
- üë®‚Äç‚öñÔ∏è Desempe√±o por juez individual
- üìã Tabla detallada de cada submission

### **Paso 3: Optimizar juez AI (Opcional)**

```bash
node lib/judge-optimizer.js
```

**Qu√© hace:**
- Analiza patrones de error sistem√°ticos
- Genera recomendaciones de ajuste
- Sugiere modificaciones al prompt
- Exporta `judge-optimization-report.json`

---

## üìä SCRIPTS DISPONIBLES

### **1. `analyze-existing-submissions.js`**

Analiza submissions existentes con AI y compara con jueces humanos.

```bash
node scripts/analyze-existing-submissions.js
```

**Opciones:**
- Edita `take: 50` para analizar m√°s o menos submissions
- Edita `setTimeout` para cambiar pausa entre requests (evitar rate limiting)

### **2. `judge-comparison-report.js`**

Genera reporte visual HTML con gr√°ficos.

```bash
node scripts/judge-comparison-report.js
```

**Salida:** `judge-ai-comparison-report.html`

### **3. `judge-optimizer.js`** (Librer√≠a)

Analiza patrones y genera recomendaciones para mejorar el AI.

```bash
node lib/judge-optimizer.js
```

**Salida:** `judge-optimization-report.json`

---

## üìà INTERPRETACI√ìN DE RESULTADOS

### **Niveles de Acuerdo**

| Nivel | Diferencia | Interpretaci√≥n | Acci√≥n |
|-------|-----------|----------------|--------|
| **EXCELENTE** | ‚â§5 pts | AI muy alineado con humanos ‚úÖ | Usar AI como segunda opini√≥n confiable |
| **BUENO** | 6-10 pts | AI aceptable ‚ö†Ô∏è | Usar AI como referencia, verificar casos extremos |
| **ACEPTABLE** | 11-15 pts | AI necesita ajustes ‚ö†Ô∏è | No usar como √∫nica autoridad, requiere supervisi√≥n |
| **POOR** | >15 pts | AI requiere reentrenamiento ‚ùå | NO usar en producci√≥n, ajustar prompt |

### **M√©tricas Clave**

#### **1. Correlaci√≥n General**
```
Diferencia promedio < 10 puntos = ‚úÖ Buen sistema
Diferencia promedio 10-15 puntos = ‚ö†Ô∏è Necesita ajustes
Diferencia promedio > 15 puntos = ‚ùå Requiere cambios importantes
```

#### **2. Sesgo Sistem√°tico**
```
Si AI > Humano consistentemente:
‚Üí El AI es demasiado generoso
‚Üí Reducir pesos de la f√≥rmula en 10-15%

Si AI < Humano consistentemente:
‚Üí El AI es demasiado estricto
‚Üí Aumentar pesos de la f√≥rmula en 10-15%
```

#### **3. Por Dificultad**
```
Verificar si el error es mayor en cierto nivel:
- easy: Debe tener menor error (trucos simples)
- expert: Mayor error aceptable (subjetividad alta)
```

#### **4. Por Juez**
```
Algunos jueces pueden ser m√°s estrictos/generosos
Comparar la alineaci√≥n de cada juez con el AI
Si un juez tiene correlaci√≥n muy baja, investigar por qu√©
```

---

## üîÑ OPTIMIZACI√ìN CONTINUA

### **Ciclo de Mejora**

```
1. Analizar submissions ‚Üí Obtener datos
2. Generar reporte ‚Üí Identificar problemas
3. Optimizar prompt ‚Üí Ajustar criterios
4. Re-analizar ‚Üí Verificar mejoras
5. Repetir cada 100 submissions o semanalmente
```

### **Ajustes Comunes**

#### **Ajuste 1: Modificar pesos de la f√≥rmula**

```javascript
// Original (demasiado alto)
const finalScore =
  (technique * 0.30) +
  (execution * 0.30) +
  (style * 0.20) +
  (difficulty * 0.20);

// Ajustado (reducir 10%)
const finalScore =
  (technique * 0.27) +
  (execution * 0.27) +
  (style * 0.18) +
  (difficulty * 0.18);
```

#### **Ajuste 2: Modificar prompt**

**Prompt demasiado estricto:**
```
"No otorgues puntos >80 a menos que sea EXCELENTE"
```

**Prompt m√°s equilibrado:**
```
"Otorga 70-80 puntos para ejecuciones limpias de trucos intermedios.
Otorga 80-90 para ejecuciones excelentes con creatividad.
Reserva 90-100 para nivel profesional."
```

#### **Ajuste 3: Agregar reglas por dificultad**

```javascript
// Agregar al prompt seg√∫n dificultad:
if (difficulty === 'easy') {
  prompt += `
  Para nivel f√°cil, s√© estricto en la forma b√°sica:
  - Ollie/kickflip deben estar muy limpios para >70 puntos
  - Peque√±os errores restan puntos significativamente
  `;
} else if (difficulty === 'expert') {
  prompt += `
  Para nivel experto, valora la complejidad:
  - Intentos de trucos complejos merecen 60+ aunque no salgan perfectos
  - La creatividad y progresi√≥n cuentan significativamente
  `;
}
```

---

## üöÄ USO EN PRODUCCI√ìN

### **Opci√≥n 1: AI como segunda opini√≥n**

```typescript
// app/api/submissions/evaluate/route.ts
export async function POST(req: Request) {
  const { submissionId } = await req.json();

  // 1. Evaluaci√≥n del juez humano
  const humanEvaluation = await getHumanEvaluation(submissionId);

  // 2. Evaluaci√≥n del AI
  const aiEvaluation = await analyzeWithAI(submissionId);

  // 3. Si hay discrepancia >15, requerir revisi√≥n
  if (Math.abs(humanEvaluation.score - aiEvaluation.score) > 15) {
    return {
      requiresReview: true,
      humanScore: humanEvaluation.score,
      aiScore: aiEvaluation.score,
      message: 'Discrepancia significativa. Requiere revisi√≥n adicional.'
    };
  }

  // 4. Promediar si est√°n cerca
  const finalScore = Math.round(
    (humanEvaluation.score + aiEvaluation.score) / 2
  );

  return { score: finalScore };
}
```

### **Opci√≥n 2: AI como pre-filtro**

```typescript
// 1. AI eval√∫a primero (r√°pido)
const aiScore = await analyzeWithAI(submissionId);

// 2. Si est√° en rango aceptable, se aprueba autom√°ticamente
if (aiScore >= 60 && aiScore <= 85) {
  return { autoApproved: true, score: aiScore };
}

// 3. Si es outlier, requiere juez humano
if (aiScore < 50 || aiScore > 90) {
  return { requiresHumanJudge: true, reason: 'Outlier detectado' };
}
```

---

## üìù EJEMPLO DE USO COMPLETO

```bash
# 1. Analizar 50 submissions recientes
node scripts/analyze-existing-submissions.js

# 2. Generar reporte visual
node scripts/judge-comparison-report.js

# 3. Abrir reporte en navegador
start judge-ai-comparison-report.html

# 4. Si la diferencia promedio es >10, optimizar
node lib/judge-optimizer.js

# 5. Aplicar ajustes recomendados al prompt en lib/judge-optimizer.js

# 6. Re-analizar para verificar mejoras
node scripts/analyze-existing-submissions.js

# 7. Comparar antes vs despu√©s
node scripts/judge-comparison-report.js
```

---

## ‚ö†Ô∏è LIMITACIONES CONOCIDAS

1. **Videos de baja calidad** ‚Üí AI no puede analizar bien si el video es oscuro, borroso o mal filmado
2. **Trucos muy t√©cnicos** ‚Üí AI puede confundir trick variations (ej: treflip vs varial heelflip)
3. **Estilo subjetivo** ‚Üí La "creatividad" es dif√≠cil de objetivar
4. **Confianza del modelo** ‚Üí Si confidence < 0.6, verificar con juez humano

**Soluci√≥n:** Implementar umbrales de confianza y verificaci√≥n humana para casos dudosos.

---

## üéØ PR√ìXIMOS PASOS

### **Fase 1: Pruebas (1 semana)**
- [ ] Analizar 50 submissions existentes
- [ ] Generar reporte inicial
- [ ] Identificar patrones de error
- [ ] Ajustar prompt inicial

### **Fase 2: Validaci√≥n (2 semanas)**
- [ ] Usar AI como segunda opini√≥n en 100 submissions nuevas
- [ ] Recopilar feedback de jueces
- [ ] Comparar vs evaluaciones humanas
- [ ] Ajustar seg√∫n feedback

### **Fase 3: Producci√≥n (despu√©s)**
- [ ] Definir protocolo de uso (AI solo, AI+humano, etc.)
- [ ] Implementar umbrales de confianza
- [ ] Crear sistema de feedback loop continuo
- [ ] Monitorear performance semanalmente

---

**¬øListo para empezar? Ejecuta:**

```bash
node scripts/analyze-existing-submissions.js
```

**¬°Genera tu primer reporte de comparaci√≥n Juez Humano vs AI! üöÄ**
